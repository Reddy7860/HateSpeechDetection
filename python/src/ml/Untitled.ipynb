{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-07 18:40:12.233250: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# from keras.engine import Model\n",
    "from keras.models import Model\n",
    "from keras.layers import Dropout, GlobalMaxPooling1D, Dense, Conv1D, MaxPooling1D, Bidirectional, Concatenate, Flatten, \\\n",
    "    GRU\n",
    "from keras.layers import LSTM\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.regularizers import L1L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_regularizer(string):\n",
    "    if string==\"none\":\n",
    "        return None\n",
    "    string_array=string.split(\"_\")\n",
    "    return L1L2(float(string_array[0]),float(string_array[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_without_branch(embedding_layers, model_descriptor:str):\n",
    "    model = Sequential()\n",
    "    if len(embedding_layers)==1:\n",
    "        model.add(embedding_layers[0])\n",
    "    else:\n",
    "        concat_embedding_layers(embedding_layers, model)\n",
    "    for layer_descriptor in model_descriptor.split(\",\"):\n",
    "        ld=layer_descriptor.split(\"=\")\n",
    "        # if layer_descriptor.endswith(\"_\"):\n",
    "            #     continue\n",
    "\n",
    "        layer_name=ld[0]\n",
    "        params=None\n",
    "        if len(ld)>1:\n",
    "            params=ld[1].split(\"-\")\n",
    "\n",
    "        if layer_name==\"dropout\":\n",
    "            model.add(Dropout(float(params[0])))\n",
    "        elif layer_name==\"lstm\":\n",
    "            if params[1]==\"True\":\n",
    "                return_seq=True\n",
    "            else:\n",
    "                return_seq=False\n",
    "            if len(params)==2:\n",
    "                model.add(LSTM(units=int(params[0]), return_sequences=return_seq))\n",
    "            if len(params)>2:\n",
    "                kernel_reg=create_regularizer(params[2])\n",
    "                activity_reg=create_regularizer(params[3])\n",
    "                if kernel_reg is not None and activity_reg is None:\n",
    "                    model.add(LSTM(units=int(params[0]), return_sequences=return_seq,\n",
    "                                       kernel_regularizer=kernel_reg))\n",
    "                elif activity_reg is not None and kernel_reg is None:\n",
    "                    model.add(LSTM(units=int(params[0]), return_sequences=return_seq,\n",
    "                                       activity_regularizer=activity_reg))\n",
    "                elif activity_reg is not None and kernel_reg is not None:\n",
    "                    model.add(LSTM(units=int(params[0]), return_sequences=return_seq,\n",
    "                                       activity_regularizer=activity_reg, kernel_regularizer=kernel_reg))\n",
    "        elif layer_name==\"gru\":\n",
    "            if params[1]==\"True\":\n",
    "                return_seq=True\n",
    "            else:\n",
    "                return_seq=False\n",
    "            if len(params)==2:\n",
    "                model.add(GRU(units=int(params[0]), return_sequences=return_seq))\n",
    "            if len(params)>2:\n",
    "                kernel_reg=create_regularizer(params[2])\n",
    "                activity_reg=create_regularizer(params[3])\n",
    "                if kernel_reg is not None and activity_reg is None:\n",
    "                    model.add(GRU(units=int(params[0]), return_sequences=return_seq,\n",
    "                                       kernel_regularizer=kernel_reg))\n",
    "                elif activity_reg is not None and kernel_reg is None:\n",
    "                    model.add(GRU(units=int(params[0]), return_sequences=return_seq,\n",
    "                                       activity_regularizer=activity_reg))\n",
    "                elif activity_reg is not None and kernel_reg is not None:\n",
    "                    model.add(GRU(units=int(params[0]), return_sequences=return_seq,\n",
    "                                       activity_regularizer=activity_reg, kernel_regularizer=kernel_reg))\n",
    "        elif layer_name==\"bilstm\":\n",
    "            model.add(Bidirectional(LSTM(units=int(params[0]), return_sequences=return_seq)))\n",
    "        elif layer_name==\"conv1d\":\n",
    "            if len(params)==2:\n",
    "                model.add(Conv1D(filters=int(params[0]),\n",
    "                             kernel_size=int(params[1]), padding='same', activation='relu'))\n",
    "            if len(params)>2:\n",
    "                kernel_reg=create_regularizer(params[2])\n",
    "                activity_reg=create_regularizer(params[3])\n",
    "                if kernel_reg is not None and activity_reg is None:\n",
    "                    model.add(Conv1D(filters=int(params[0]),\n",
    "                             kernel_size=int(params[1]), padding='same', activation='relu', kernel_regularizer=kernel_reg))\n",
    "                elif activity_reg is not None and kernel_reg is None:\n",
    "                    model.add(Conv1D(filters=int(params[0]),\n",
    "                             kernel_size=int(params[1]), padding='same', activation='relu',activity_regularizer=activity_reg))\n",
    "                elif activity_reg is not None and kernel_reg is not None:\n",
    "                    model.add(Conv1D(filters=int(params[0]),\n",
    "                             kernel_size=int(params[1]), padding='same', activation='relu', kernel_regularizer=kernel_reg, activity_regularizer=activity_reg))\n",
    "        elif layer_name==\"maxpooling1d\":\n",
    "            model.add(MaxPooling1D(pool_size=int(params[0])))\n",
    "        elif layer_name==\"gmaxpooling1d\":\n",
    "            model.add(GlobalMaxPooling1D())\n",
    "        elif layer_name==\"dense\":\n",
    "            if len(params)==2:\n",
    "                model.add(Dense(int(params[0]), activation=params[1]))\n",
    "            elif len(params)>2:\n",
    "                kernel_reg=create_regularizer(params[2])\n",
    "                activity_reg=create_regularizer(params[3])\n",
    "                if kernel_reg is not None and activity_reg is None:\n",
    "                    model.add(Dense(int(params[0]), activation=params[1],\n",
    "                                    kernel_regularizer=kernel_reg))\n",
    "                elif activity_reg is not None and kernel_reg is None:\n",
    "                    model.add(Dense(int(params[0]), activation=params[1],\n",
    "                                    activity_regularizer=activity_reg))\n",
    "                elif activity_reg is not None and kernel_reg is not None:\n",
    "                    model.add(Dense(int(params[0]), activation=params[1],\n",
    "                                    activity_regularizer=activity_reg,\n",
    "                                        kernel_regularizer=kernel_reg))\n",
    "            else:\n",
    "                model.add(Dense(int(params[0])))\n",
    "        elif layer_name==\"flatten\":\n",
    "            model.add(Flatten())\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_final_model_with_concat_cnn(embedding_layers, model_descriptor:str):\n",
    "    #model_desc=(conv1d=100-[3,4,5],so),lstm=100-True,gmaxpooling1d,dense=2-softmax\n",
    "    target_grams=model_descriptor[model_descriptor.index(\"[\")+1: model_descriptor.index(\"]\")]\n",
    "\n",
    "    submodels = []\n",
    "    if \",so\" in model_descriptor:\n",
    "        skip_layers_only=True\n",
    "    else:\n",
    "        skip_layers_only=False\n",
    "    for n in target_grams.split(\",\"):\n",
    "        for mod in create_skipped_conv1d_submodels(embedding_layers, int(n), skip_layers_only):\n",
    "            submodels.append(mod)\n",
    "\n",
    "    submodel_outputs = [model.output for model in submodels]\n",
    "    if len(submodel_outputs)>1:\n",
    "        x = Concatenate(axis=1)(submodel_outputs)\n",
    "    else:\n",
    "        x= submodel_outputs[0]\n",
    "    parallel_layers=Model(inputs=embedding_layers[0].input, outputs=x)\n",
    "    #print(\"submodel:\")\n",
    "    #parallel_layers.summary()\n",
    "    #print(\"\\n\")\n",
    "\n",
    "    outter_model_descriptor=model_descriptor[model_descriptor.index(\")\")+2:]\n",
    "    big_model = Sequential()\n",
    "    big_model.add(parallel_layers)\n",
    "    for layer_descriptor in outter_model_descriptor.split(\",\"):\n",
    "        ld=layer_descriptor.split(\"=\")\n",
    "\n",
    "        layer_name=ld[0]\n",
    "        params=None\n",
    "        if len(ld)>1:\n",
    "            params=ld[1].split(\"-\")\n",
    "\n",
    "        if layer_name==\"dropout\":\n",
    "            big_model.add(Dropout(float(params[0])))\n",
    "        elif layer_name==\"lstm\":\n",
    "            if params[1]==\"True\":\n",
    "                return_seq=True\n",
    "            else:\n",
    "                return_seq=False\n",
    "            if len(params)==2:\n",
    "                big_model.add(LSTM(units=int(params[0]), return_sequences=return_seq))\n",
    "            if len(params)>2:\n",
    "                kernel_reg=create_regularizer(params[2])\n",
    "                activity_reg=create_regularizer(params[3])\n",
    "                if kernel_reg is not None and activity_reg is None:\n",
    "                    big_model.add(LSTM(units=int(params[0]), return_sequences=return_seq,\n",
    "                                       kernel_regularizer=kernel_reg))\n",
    "                elif activity_reg is not None and kernel_reg is None:\n",
    "                    big_model.add(LSTM(units=int(params[0]), return_sequences=return_seq,\n",
    "                                       activity_regularizer=activity_reg))\n",
    "                elif activity_reg is not None and kernel_reg is not None:\n",
    "                    big_model.add(LSTM(units=int(params[0]), return_sequences=return_seq,\n",
    "                                       activity_regularizer=activity_reg, kernel_regularizer=kernel_reg))\n",
    "\n",
    "        elif layer_name==\"gru\":\n",
    "            if params[1]==\"True\":\n",
    "                return_seq=True\n",
    "            else:\n",
    "                return_seq=False\n",
    "            if len(params)==2:\n",
    "                big_model.add(GRU(units=int(params[0]), return_sequences=return_seq))\n",
    "            if len(params)>2:\n",
    "                kernel_reg=create_regularizer(params[2])\n",
    "                activity_reg=create_regularizer(params[3])\n",
    "                if kernel_reg is not None and activity_reg is None:\n",
    "                    big_model.add(GRU(units=int(params[0]), return_sequences=return_seq,\n",
    "                                       kernel_regularizer=kernel_reg))\n",
    "                elif activity_reg is not None and kernel_reg is None:\n",
    "                    big_model.add(GRU(units=int(params[0]), return_sequences=return_seq,\n",
    "                                       activity_regularizer=activity_reg))\n",
    "                elif activity_reg is not None and kernel_reg is not None:\n",
    "                    big_model.add(GRU(units=int(params[0]), return_sequences=return_seq,\n",
    "                                       activity_regularizer=activity_reg, kernel_regularizer=kernel_reg))\n",
    "        elif layer_name==\"bilstm\":\n",
    "            big_model.add(Bidirectional(LSTM(units=int(params[0]), return_sequences=return_seq)))\n",
    "        elif layer_name==\"conv1d\":\n",
    "            if len(params)==2:\n",
    "                big_model.add(Conv1D(filters=int(params[0]),\n",
    "                             kernel_size=int(params[1]), padding='same', activation='relu'))\n",
    "            if len(params)>2:\n",
    "                kernel_reg=create_regularizer(params[2])\n",
    "                activity_reg=create_regularizer(params[3])\n",
    "                if kernel_reg is not None and activity_reg is None:\n",
    "                    big_model.add(Conv1D(filters=int(params[0]),\n",
    "                             kernel_size=int(params[1]), padding='same', activation='relu', kernel_regularizer=kernel_reg))\n",
    "                elif activity_reg is not None and kernel_reg is None:\n",
    "                    big_model.add(Conv1D(filters=int(params[0]),\n",
    "                             kernel_size=int(params[1]), padding='same', activation='relu',activity_regularizer=activity_reg))\n",
    "                elif activity_reg is not None and kernel_reg is not None:\n",
    "                    big_model.add(Conv1D(filters=int(params[0]),\n",
    "                             kernel_size=int(params[1]), padding='same', activation='relu', kernel_regularizer=kernel_reg, activity_regularizer=activity_reg))\n",
    "        elif layer_name==\"maxpooling1d\":\n",
    "            big_model.add(MaxPooling1D(pool_size=int(params[0])))\n",
    "        elif layer_name==\"gmaxpooling1d\":\n",
    "            big_model.add(GlobalMaxPooling1D())\n",
    "        elif layer_name==\"dense\":\n",
    "            if len(params)==2:\n",
    "                big_model.add(Dense(int(params[0]), activation=params[1]))\n",
    "            elif len(params)>2:\n",
    "                kernel_reg=create_regularizer(params[2])\n",
    "                activity_reg=create_regularizer(params[3])\n",
    "                if kernel_reg is not None and activity_reg is None:\n",
    "                    big_model.add(Dense(int(params[0]), activation=params[1],\n",
    "                                    kernel_regularizer=kernel_reg))\n",
    "                elif activity_reg is not None and kernel_reg is None:\n",
    "                    big_model.add(Dense(int(params[0]), activation=params[1],\n",
    "                                    activity_regularizer=activity_reg))\n",
    "                elif activity_reg is not None and kernel_reg is not None:\n",
    "                    big_model.add(Dense(int(params[0]), activation=params[1],\n",
    "                                    activity_regularizer=activity_reg,\n",
    "                                        kernel_regularizer=kernel_reg))\n",
    "            else:\n",
    "                big_model.add(Dense(int(params[0])))\n",
    "        elif layer_name==\"flatten\":\n",
    "            big_model.add(Flatten())\n",
    "\n",
    "    big_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    #big_model.summary()\n",
    "\n",
    "    return big_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_skipped_conv1d_submodels(embedding_layers, cnn_ks, skip_layer_only:bool):\n",
    "    models=[]\n",
    "\n",
    "    conv_layers=[]\n",
    "    if cnn_ks<3:\n",
    "        if not skip_layer_only:\n",
    "            conv1d_3=Conv1D(filters=100,kernel_size=cnn_ks, padding='same', activation='relu')\n",
    "            conv_layers.append(conv1d_3)\n",
    "    elif cnn_ks==3:\n",
    "        if not skip_layer_only:\n",
    "            conv1d_3=Conv1D(filters=100,kernel_size=3, padding='same', activation='relu')\n",
    "            conv_layers.append(conv1d_3)\n",
    "\n",
    "        #2skip1\n",
    "        ks_and_masks=generate_ks_and_masks(2, 1)\n",
    "        for mask in ks_and_masks[1]:\n",
    "            conv_layers.append(SkipConv1D(filters=100,\n",
    "                          kernel_size=int(ks_and_masks[0]), validGrams=mask,\n",
    "                          padding='same', activation='relu'))\n",
    "        add_skipped_conv1d_submodel_other_layers(conv_layers,embedding_layers,models)\n",
    "\n",
    "    elif cnn_ks==4:\n",
    "        if not skip_layer_only:\n",
    "            conv1d_4=Conv1D(filters=100,kernel_size=4, padding='same', activation='relu')\n",
    "            conv_layers.append(conv1d_4)\n",
    "\n",
    "        #2skip2\n",
    "        ks_and_masks=generate_ks_and_masks(2, 2)\n",
    "        for mask in ks_and_masks[1]:\n",
    "            conv_layers.append(SkipConv1D(filters=100,\n",
    "                          kernel_size=int(ks_and_masks[0]), validGrams=mask,\n",
    "                          padding='same', activation='relu'))\n",
    "        #3skip1\n",
    "        ks_and_masks=generate_ks_and_masks(3, 1)\n",
    "        for mask in ks_and_masks[1]:\n",
    "            conv_layers.append(SkipConv1D(filters=100,\n",
    "                          kernel_size=int(ks_and_masks[0]), validGrams=mask,\n",
    "                          padding='same', activation='relu'))\n",
    "        add_skipped_conv1d_submodel_other_layers(conv_layers,embedding_layers,models)\n",
    "\n",
    "\n",
    "    elif cnn_ks==5:\n",
    "        if not skip_layer_only:\n",
    "            conv1d_5=Conv1D(filters=100,kernel_size=5, padding='same', activation='relu')\n",
    "            conv_layers.append(conv1d_5)\n",
    "        #2skip3\n",
    "        ks_and_masks=generate_ks_and_masks(2, 3)\n",
    "        for mask in ks_and_masks[1]:\n",
    "            conv_layers.append(SkipConv1D(filters=100,\n",
    "                          kernel_size=int(ks_and_masks[0]), validGrams=mask,\n",
    "                          padding='same', activation='relu'))\n",
    "        #3skip2\n",
    "        ks_and_masks=generate_ks_and_masks(3, 2)\n",
    "        for mask in ks_and_masks[1]:\n",
    "            conv_layers.append(SkipConv1D(filters=100,\n",
    "                          kernel_size=int(ks_and_masks[0]), validGrams=mask,\n",
    "                          padding='same', activation='relu'))\n",
    "        #4skip1\n",
    "        ks_and_masks=generate_ks_and_masks(4, 1)\n",
    "        for mask in ks_and_masks[1]:\n",
    "            conv_layers.append(SkipConv1D(filters=100,\n",
    "                          kernel_size=int(ks_and_masks[0]), validGrams=mask,\n",
    "                          padding='same', activation='relu'))\n",
    "        #3dilate1\n",
    "        conv_layers.append(Conv1D(filters=100,\n",
    "                          kernel_size=3, dilation_rate=1,\n",
    "                          padding='same', activation='relu'))\n",
    "        add_skipped_conv1d_submodel_other_layers(conv_layers,embedding_layers,models)\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_skipped_conv1d_submodel_other_layers(conv_layers, embedding_layers,models:list):\n",
    "    for conv_layer in conv_layers:\n",
    "        model = Sequential()\n",
    "        if len(embedding_layers)==1:\n",
    "            model.add(embedding_layers[0])\n",
    "        else:\n",
    "            concat_embedding_layers(embedding_layers, model)\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(conv_layer)\n",
    "        model.add(MaxPooling1D(pool_size=4))\n",
    "        models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#warning: concat embedding layers currently does not work!\n",
    "def concat_embedding_layers(embedding_layers, big_model):\n",
    "    submodels = []\n",
    "\n",
    "    for el in embedding_layers:\n",
    "        m = Sequential()\n",
    "        m.add(el)\n",
    "        submodels.append(m)\n",
    "\n",
    "    submodel_outputs = [model.output for model in submodels]\n",
    "    if len(submodel_outputs) > 1:\n",
    "        x = Concatenate(axis=2)(submodel_outputs)\n",
    "    else:\n",
    "        x = submodel_outputs[0]\n",
    "\n",
    "    parallel_layers = Model(inputs=[embedding_layers[0].input, embedding_layers[1].input], outputs=x)\n",
    "    big_model.add(parallel_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_with_branch(embedding_layers, model_descriptor:str):\n",
    "    \"sub_conv[2,3,4](dropout=0.2,conv1d=100-v,)\"\n",
    "    submod_str_start=model_descriptor.index(\"sub_conv\")\n",
    "    submod_str_end=model_descriptor.index(\")\")\n",
    "    submod_str=model_descriptor[submod_str_start: submod_str_end]\n",
    "\n",
    "    kernel_str=submod_str[submod_str.index(\"[\")+1: submod_str.index(\"]\")]\n",
    "    dilation_rates=[]\n",
    "    if \"{\" in submod_str:\n",
    "        dilation_str=submod_str[submod_str.index(\"{\")+1:submod_str.index(\"}\")]\n",
    "        dilation_rates=dilation_str.split(\",\")\n",
    "    skipgrams=[]\n",
    "    if \"<\" in submod_str: #skipconv1d\n",
    "        skipgram_str=submod_str[submod_str.index(\"<\")+1:submod_str.index(\">\")]\n",
    "        skipgrams=skipgram_str.split(\",\")\n",
    "    submod_layer_descriptor = submod_str[submod_str.index(\"(\")+1:]\n",
    "    submodels = []\n",
    "    for ks in kernel_str.split(\",\"):\n",
    "        submodels.append(create_submodel(embedding_layers, submod_layer_descriptor, ks))\n",
    "\n",
    "    for dr in dilation_rates:\n",
    "        for ks in kernel_str.split(\",\"):\n",
    "            submodels.append(create_submodel(embedding_layers, submod_layer_descriptor, ks, dr))\n",
    "\n",
    "    for sk in skipgrams:\n",
    "        for ks in kernel_str.split(\",\"):\n",
    "            skipconv_submodels=(\n",
    "                create_submodel_with_skipconv1d(embedding_layers, submod_layer_descriptor, int(ks),int(sk)))\n",
    "            for sm in skipconv_submodels:\n",
    "                submodels.append(sm)\n",
    "\n",
    "    submodel_outputs = [model.output for model in submodels]\n",
    "    if len(submodel_outputs)>1:\n",
    "        x = Concatenate(axis=1)(submodel_outputs)\n",
    "    else:\n",
    "        x=submodel_outputs[0]\n",
    "\n",
    "    parallel_layers=Model(inputs=embedding_layers[0].input, outputs=x)\n",
    "    #Howprint(\"submodel:\")\n",
    "    #parallel_layers.summary()\n",
    "    #print(\"\\n\")\n",
    "\n",
    "    outter_model_descriptor=model_descriptor[model_descriptor.index(\")\")+2:]\n",
    "    big_model = Sequential()\n",
    "    big_model.add(parallel_layers)\n",
    "    for layer_descriptor in outter_model_descriptor.split(\",\"):\n",
    "        ld=layer_descriptor.split(\"=\")\n",
    "\n",
    "        layer_name=ld[0]\n",
    "        params=None\n",
    "        if len(ld)>1:\n",
    "            params=ld[1].split(\"-\")\n",
    "\n",
    "        if layer_name==\"dropout\":\n",
    "            big_model.add(Dropout(float(params[0])))\n",
    "        elif layer_name==\"lstm\":\n",
    "            if params[1]==\"True\":\n",
    "                return_seq=True\n",
    "            else:\n",
    "                return_seq=False\n",
    "            big_model.add(LSTM(units=int(params[0]), return_sequences=return_seq))\n",
    "        elif layer_name==\"gru\":\n",
    "            if params[1]==\"True\":\n",
    "                return_seq=True\n",
    "            else:\n",
    "                return_seq=False\n",
    "            big_model.add(GRU(units=int(params[0]), return_sequences=return_seq))\n",
    "        elif layer_name==\"bilstm\":\n",
    "            if params[1]==\"True\":\n",
    "                return_seq=True\n",
    "            else:\n",
    "                return_seq=False\n",
    "            big_model.add(Bidirectional(LSTM(units=int(params[0]), return_sequences=return_seq)))\n",
    "        elif layer_name==\"conv1d\":\n",
    "            if len(params)==2:\n",
    "                big_model.add(Conv1D(filters=int(params[0]),\n",
    "                             kernel_size=int(params[1]), padding='same', activation='relu'))\n",
    "            elif len(params)==3:\n",
    "                print(\"dilated cnn\")\n",
    "                big_model.add(Conv1D(filters=int(params[0]),\n",
    "                             kernel_size=int(params[1]), dilation_rate=int(params[2]),padding='same', activation='relu'))\n",
    "        elif layer_name==\"maxpooling1d\":\n",
    "            big_model.add(MaxPooling1D(pool_size=int(params[0])))\n",
    "        elif layer_name==\"gmaxpooling1d\":\n",
    "            big_model.add(GlobalMaxPooling1D())\n",
    "        elif layer_name == \"dense\":\n",
    "            if len(params) == 2:\n",
    "                big_model.add(Dense(int(params[0]), activation=params[1]))\n",
    "            elif len(params) > 2:\n",
    "                kernel_reg = create_regularizer(params[2])\n",
    "                activity_reg = create_regularizer(params[3])\n",
    "                if kernel_reg is not None and activity_reg is None:\n",
    "                    big_model.add(Dense(int(params[0]), activation=params[1],\n",
    "                                        kernel_regularizer=kernel_reg))\n",
    "                elif activity_reg is not None and kernel_reg is None:\n",
    "                    big_model.add(Dense(int(params[0]), activation=params[1],\n",
    "                                        activity_regularizer=activity_reg))\n",
    "                elif activity_reg is not None and kernel_reg is not None:\n",
    "                    big_model.add(Dense(int(params[0]), activation=params[1],\n",
    "                                        activity_regularizer=activity_reg,\n",
    "                                        kernel_regularizer=kernel_reg))\n",
    "        elif layer_name==\"flatten\":\n",
    "            big_model.add(Flatten())\n",
    "\n",
    "    big_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    #big_model.summary()\n",
    "\n",
    "    return big_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submodel(embedding_layers, submod_layer_descriptor, cnn_ks, cnn_dilation=None):\n",
    "    model = Sequential()\n",
    "    if len(embedding_layers)==1:\n",
    "        model.add(embedding_layers[0])\n",
    "    else:\n",
    "        concat_embedding_layers(embedding_layers, model)\n",
    "    for layer_descriptor in submod_layer_descriptor.split(\",\"):\n",
    "        if \"=\" not in layer_descriptor:\n",
    "            continue\n",
    "        ld=layer_descriptor.split(\"=\")\n",
    "\n",
    "        layer_name=ld[0]\n",
    "        params=None\n",
    "        if len(ld)>1:\n",
    "            params=ld[1].split(\"-\")\n",
    "\n",
    "        if layer_name==\"dropout\":\n",
    "            model.add(Dropout(float(params[0])))\n",
    "        elif layer_name==\"lstm\":\n",
    "            if params[1]==\"True\":\n",
    "                return_seq=True\n",
    "            else:\n",
    "                return_seq=False\n",
    "            model.add(LSTM(units=int(params[0]), return_sequences=return_seq))\n",
    "        elif layer_name==\"gru\":\n",
    "            if params[1]==\"True\":\n",
    "                return_seq=True\n",
    "            else:\n",
    "                return_seq=False\n",
    "            model.add(GRU(units=int(params[0]), return_sequences=return_seq))\n",
    "        elif layer_name==\"bilstm\":\n",
    "            if params[1]==\"True\":\n",
    "                return_seq=True\n",
    "            else:\n",
    "                return_seq=False\n",
    "            model.add(Bidirectional(LSTM(units=int(params[0]), return_sequences=return_seq)))\n",
    "        elif layer_name==\"conv1d\":\n",
    "            if cnn_dilation is None:\n",
    "                model.add(Conv1D(filters=int(params[0]),\n",
    "                             kernel_size=int(cnn_ks), padding='same', activation='relu'))\n",
    "            else:\n",
    "                model.add(Conv1D(filters=int(params[0]),\n",
    "                             kernel_size=int(cnn_ks), dilation_rate=int(cnn_dilation),\n",
    "                                 padding='same', activation='relu'))\n",
    "\n",
    "        elif layer_name==\"maxpooling1d\":\n",
    "            size=params[0]\n",
    "            if size==\"v\":\n",
    "                size=int(cnn_ks)\n",
    "            else:\n",
    "                size=int(params[0])\n",
    "            model.add(MaxPooling1D(pool_size=size))\n",
    "        elif layer_name==\"gmaxpooling1d\":\n",
    "            model.add(GlobalMaxPooling1D())\n",
    "        elif layer_name==\"dense\":\n",
    "            model.add(Dense(int(params[0]), activation=params[1]))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ks_and_masks(target_cnn_ks, skip):\n",
    "    masks=[]\n",
    "    real_cnn_ks=target_cnn_ks+skip\n",
    "    for gap_index in range(1, real_cnn_ks):\n",
    "        mask=[]\n",
    "        for ones in range(0,gap_index):\n",
    "            mask.append(1)\n",
    "        for zeros in range(gap_index,gap_index+skip):\n",
    "            if zeros<real_cnn_ks:\n",
    "                mask.append(0)\n",
    "        for ones in range(gap_index+skip, real_cnn_ks):\n",
    "            if ones <real_cnn_ks:\n",
    "                mask.append(1)\n",
    "\n",
    "        if mask[len(mask)-1]!=0:\n",
    "            masks.append(mask)\n",
    "    return [real_cnn_ks,masks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submodel_with_skipconv1d(embedding_layer, submod_layer_descriptor, target_cnn_ks, skip\n",
    "                                    ):\n",
    "    submodels=[]\n",
    "    ks_and_masks=generate_ks_and_masks(target_cnn_ks, skip)\n",
    "    for mask in ks_and_masks[1]:\n",
    "        model = Sequential()\n",
    "        model.add(embedding_layer)\n",
    "        for layer_descriptor in submod_layer_descriptor.split(\",\"):\n",
    "            if layer_descriptor.endswith(\"_\"):\n",
    "                continue\n",
    "            ld=layer_descriptor.split(\"=\")\n",
    "\n",
    "            layer_name=ld[0]\n",
    "            params=None\n",
    "            if len(ld)>1:\n",
    "                params=ld[1].split(\"-\")\n",
    "\n",
    "            if layer_name==\"dropout\":\n",
    "                model.add(Dropout(float(params[0])))\n",
    "            elif layer_name==\"lstm\":\n",
    "                if params[1]==\"True\":\n",
    "                    return_seq=True\n",
    "                else:\n",
    "                    return_seq=False\n",
    "                model.add(LSTM(units=int(params[0]), return_sequences=return_seq))\n",
    "            elif layer_name==\"gru\":\n",
    "                if params[1]==\"True\":\n",
    "                    return_seq=True\n",
    "                else:\n",
    "                    return_seq=False\n",
    "                model.add(GRU(units=int(params[0]), return_sequences=return_seq))\n",
    "            elif layer_name==\"bilstm\":\n",
    "                if params[1]==\"True\":\n",
    "                    return_seq=True\n",
    "                else:\n",
    "                    return_seq=False\n",
    "                model.add(Bidirectional(LSTM(units=int(params[0]), return_sequences=return_seq)))\n",
    "            elif layer_name==\"conv1d\":\n",
    "                model.add(SkipConv1D(filters=int(params[0]),\n",
    "                          kernel_size=int(ks_and_masks[0]), validGrams=mask,\n",
    "                          padding='same', activation='relu'))\n",
    "\n",
    "            elif layer_name==\"maxpooling1d\":\n",
    "                size=params[0]\n",
    "                if size==\"v\":\n",
    "                    size=int(ks_and_masks[0])\n",
    "                else:\n",
    "                    size=int(params[0])\n",
    "                model.add(MaxPooling1D(pool_size=size))\n",
    "            elif layer_name==\"gmaxpooling1d\":\n",
    "                model.add(GlobalMaxPooling1D())\n",
    "            elif layer_name==\"dense\":\n",
    "                model.add(Dense(int(params[0]), activation=params[1]))\n",
    "        submodels.append(model)\n",
    "    return submodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_type1(embedding_layer):#start from simple model\n",
    "    # pass the embedding layer with model parameters of lstm\n",
    "    return create_model_without_branch(embedding_layer, \"dropout=0.2,lstm=100-True,gmaxpooling1d,\"\n",
    "                                                        \"dropout=0.2,dense=2-softmax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_conv_lstm_type1(embedding_layer):\n",
    "    # pass the embedding layer and model parameters of convolution\n",
    "    return create_model_without_branch(embedding_layer,\n",
    "                                       \"dropout=0.2,conv1d=100-4,maxpooling1d=4,\"\n",
    "                                       \"lstm=100-True,gmaxpooling1d,dense=2-softmax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_without_branch(embedding_layers, model_descriptor:str):\n",
    "    model = Sequential()\n",
    "    print(len(embedding_layers))\n",
    "    if len(embedding_layers)==1:\n",
    "        model.add(embedding_layers[0])\n",
    "    else:\n",
    "        concat_embedding_layers(embedding_layers, model)\n",
    "        \n",
    "    \n",
    "    for layer_descriptor in model_descriptor.split(\",\"):\n",
    "        ld=layer_descriptor.split(\"=\")\n",
    "        # if layer_descriptor.endswith(\"_\"):\n",
    "            #     continue\n",
    "\n",
    "        layer_name=ld[0]\n",
    "        params=None\n",
    "        if len(ld)>1:\n",
    "            params=ld[1].split(\"-\")\n",
    "\n",
    "        if layer_name==\"dropout\":\n",
    "            model.add(Dropout(float(params[0])))\n",
    "        elif layer_name==\"lstm\":\n",
    "            if params[1]==\"True\":\n",
    "                return_seq=True\n",
    "            else:\n",
    "                return_seq=False\n",
    "            if len(params)==2:\n",
    "                model.add(LSTM(units=int(params[0]), return_sequences=return_seq))\n",
    "            if len(params)>2:\n",
    "                kernel_reg=create_regularizer(params[2])\n",
    "                activity_reg=create_regularizer(params[3])\n",
    "                if kernel_reg is not None and activity_reg is None:\n",
    "                    model.add(LSTM(units=int(params[0]), return_sequences=return_seq,\n",
    "                                       kernel_regularizer=kernel_reg))\n",
    "                elif activity_reg is not None and kernel_reg is None:\n",
    "                    model.add(LSTM(units=int(params[0]), return_sequences=return_seq,\n",
    "                                       activity_regularizer=activity_reg))\n",
    "                elif activity_reg is not None and kernel_reg is not None:\n",
    "                    model.add(LSTM(units=int(params[0]), return_sequences=return_seq,\n",
    "                                       activity_regularizer=activity_reg, kernel_regularizer=kernel_reg))\n",
    "        elif layer_name==\"gru\":\n",
    "            if params[1]==\"True\":\n",
    "                return_seq=True\n",
    "            else:\n",
    "                return_seq=False\n",
    "            if len(params)==2:\n",
    "                model.add(GRU(units=int(params[0]), return_sequences=return_seq))\n",
    "            if len(params)>2:\n",
    "                kernel_reg=create_regularizer(params[2])\n",
    "                activity_reg=create_regularizer(params[3])\n",
    "                if kernel_reg is not None and activity_reg is None:\n",
    "                    model.add(GRU(units=int(params[0]), return_sequences=return_seq,\n",
    "                                       kernel_regularizer=kernel_reg))\n",
    "                elif activity_reg is not None and kernel_reg is None:\n",
    "                    model.add(GRU(units=int(params[0]), return_sequences=return_seq,\n",
    "                                       activity_regularizer=activity_reg))\n",
    "                elif activity_reg is not None and kernel_reg is not None:\n",
    "                    model.add(GRU(units=int(params[0]), return_sequences=return_seq,\n",
    "                                       activity_regularizer=activity_reg, kernel_regularizer=kernel_reg))\n",
    "        elif layer_name==\"bilstm\":\n",
    "            model.add(Bidirectional(LSTM(units=int(params[0]), return_sequences=return_seq)))\n",
    "        elif layer_name==\"conv1d\":\n",
    "            if len(params)==2:\n",
    "                model.add(Conv1D(filters=int(params[0]),\n",
    "                             kernel_size=int(params[1]), padding='same', activation='relu'))\n",
    "            if len(params)>2:\n",
    "                kernel_reg=create_regularizer(params[2])\n",
    "                activity_reg=create_regularizer(params[3])\n",
    "                if kernel_reg is not None and activity_reg is None:\n",
    "                    model.add(Conv1D(filters=int(params[0]),\n",
    "                             kernel_size=int(params[1]), padding='same', activation='relu', kernel_regularizer=kernel_reg))\n",
    "                elif activity_reg is not None and kernel_reg is None:\n",
    "                    model.add(Conv1D(filters=int(params[0]),\n",
    "                             kernel_size=int(params[1]), padding='same', activation='relu',activity_regularizer=activity_reg))\n",
    "                elif activity_reg is not None and kernel_reg is not None:\n",
    "                    model.add(Conv1D(filters=int(params[0]),\n",
    "                             kernel_size=int(params[1]), padding='same', activation='relu', kernel_regularizer=kernel_reg, activity_regularizer=activity_reg))\n",
    "        elif layer_name==\"maxpooling1d\":\n",
    "            model.add(MaxPooling1D(pool_size=int(params[0])))\n",
    "        elif layer_name==\"gmaxpooling1d\":\n",
    "            model.add(GlobalMaxPooling1D())\n",
    "        elif layer_name==\"dense\":\n",
    "            if len(params)==2:\n",
    "                model.add(Dense(int(params[0]), activation=params[1]))\n",
    "            elif len(params)>2:\n",
    "                kernel_reg=create_regularizer(params[2])\n",
    "                activity_reg=create_regularizer(params[3])\n",
    "                if kernel_reg is not None and activity_reg is None:\n",
    "                    model.add(Dense(int(params[0]), activation=params[1],\n",
    "                                    kernel_regularizer=kernel_reg))\n",
    "                elif activity_reg is not None and kernel_reg is None:\n",
    "                    model.add(Dense(int(params[0]), activation=params[1],\n",
    "                                    activity_regularizer=activity_reg))\n",
    "                elif activity_reg is not None and kernel_reg is not None:\n",
    "                    model.add(Dense(int(params[0]), activation=params[1],\n",
    "                                    activity_regularizer=activity_reg,\n",
    "                                        kernel_regularizer=kernel_reg))\n",
    "            else:\n",
    "                model.add(Dense(int(params[0])))\n",
    "        elif layer_name==\"flatten\":\n",
    "            model.add(Flatten())\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "The added layer must be an instance of class Layer. Received: layer=200 of type <class 'int'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Embedding\n\u001b[1;32m      2\u001b[0m e \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m50\u001b[39m]\n\u001b[0;32m----> 4\u001b[0m \u001b[43mcreate_model_without_branch\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                       \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdropout=0.2,conv1d=100-4,maxpooling1d=4,\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                       \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlstm=100-True,gmaxpooling1d,dense=2-softmax\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36mcreate_model_without_branch\u001b[0;34m(embedding_layers, model_descriptor)\u001b[0m\n\u001b[1;32m      5\u001b[0m     model\u001b[38;5;241m.\u001b[39madd(embedding_layers[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m----> 7\u001b[0m     \u001b[43mconcat_embedding_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_descriptor \u001b[38;5;129;01min\u001b[39;00m model_descriptor\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      9\u001b[0m     ld\u001b[38;5;241m=\u001b[39mlayer_descriptor\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mconcat_embedding_layers\u001b[0;34m(embedding_layers, big_model)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m el \u001b[38;5;129;01min\u001b[39;00m embedding_layers:\n\u001b[1;32m      6\u001b[0m     m \u001b[38;5;241m=\u001b[39m Sequential()\n\u001b[0;32m----> 7\u001b[0m     \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     submodels\u001b[38;5;241m.\u001b[39mappend(m)\n\u001b[1;32m     10\u001b[0m submodel_outputs \u001b[38;5;241m=\u001b[39m [model\u001b[38;5;241m.\u001b[39moutput \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m submodels]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/trackable/base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/keras/engine/sequential.py:183\u001b[0m, in \u001b[0;36mSequential.add\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    181\u001b[0m         layer \u001b[38;5;241m=\u001b[39m functional\u001b[38;5;241m.\u001b[39mModuleWrapper(layer)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 183\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    184\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe added layer must be an instance of class Layer. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    185\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: layer=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(layer)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    186\u001b[0m     )\n\u001b[1;32m    188\u001b[0m tf_utils\u001b[38;5;241m.\u001b[39massert_no_legacy_layers([layer])\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_layer_name_unique(layer):\n",
      "\u001b[0;31mTypeError\u001b[0m: The added layer must be an instance of class Layer. Received: layer=200 of type <class 'int'>."
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding\n",
    "e = [200, 32, 50]\n",
    "\n",
    "create_model_without_branch(e,\n",
    "                                       \"dropout=0.2,conv1d=100-4,maxpooling1d=4,\"\n",
    "                                       \"lstm=100-True,gmaxpooling1d,dense=2-softmax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a 1D convolution that skips some entries\n",
    "class SkipConv1D(Conv1D):\n",
    "\n",
    "    #in the init, let's just add a parameter to tell which grams to skip\n",
    "    def __init__(self, validGrams, **kwargs):\n",
    "\n",
    "        #for this example, I'm assuming validGrams is a list\n",
    "        #it should contain zeros and ones, where 0's go on the skip positions\n",
    "        #example: [1,1,0,1] will skip the third gram in the window of 4 grams\n",
    "        assert len(validGrams) == kwargs.get('kernel_size')\n",
    "        self.validGrams = K.reshape(K.constant(validGrams),(len(validGrams),1,1))\n",
    "            #the chosen shape matches the dimensions of the kernel\n",
    "            #the first dimension is the kernel size, the others are input and ouptut channels\n",
    "\n",
    "\n",
    "        #initialize the regular conv layer:\n",
    "        super(SkipConv1D,self).__init__(**kwargs)\n",
    "\n",
    "        #here, the filters, size, etc, go inside kwargs, so you should use them named\n",
    "        #but you may make them explicit in this __init__ definition\n",
    "        #if you think it's more comfortable to use it like this\n",
    "\n",
    "\n",
    "    #in the build method, let's replace the original kernel:\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        #build as the original layer:\n",
    "        super(SkipConv1D,self).build(input_shape)\n",
    "\n",
    "        #replace the kernel\n",
    "        self.originalKernel = self.kernel\n",
    "        self.kernel = self.validGrams * self.originalKernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'kwargs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m validGrams \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(validGrams) \u001b[38;5;241m==\u001b[39m \u001b[43mkwargs\u001b[49m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkernel_size\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'kwargs' is not defined"
     ]
    }
   ],
   "source": [
    "validGrams = []\n",
    "assert len(validGrams) == kwargs.get('kernel_size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "list1 =[1,2,3]\n",
    "list2 = [4,5,6]\n",
    "\n",
    "\n",
    "list_of_words = []\n",
    "\n",
    "list_of_words.append(list1)\n",
    "list_of_words.append(list2)\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "print(list(chain(*list_of_words)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.random.rand(500, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.29866155, 0.14650654, 0.10698411, ..., 0.5727265 , 0.20649698,\n",
       "        0.40654719],\n",
       "       [0.77887458, 0.89994874, 0.56103417, ..., 0.6457272 , 0.51542677,\n",
       "        0.15115571],\n",
       "       [0.68065544, 0.69256783, 0.66005335, ..., 0.16815034, 0.13071318,\n",
       "        0.81958088],\n",
       "       ...,\n",
       "       [0.74020871, 0.90025545, 0.62654903, ..., 0.9408762 , 0.68657259,\n",
       "        0.99569882],\n",
       "       [0.47567778, 0.73380077, 0.95006517, ..., 0.55016898, 0.38139813,\n",
       "        0.11328876],\n",
       "       [0.98713556, 0.1400213 , 0.72016226, ..., 0.37340145, 0.4961315 ,\n",
       "        0.80079332]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'aa'}\n"
     ]
    }
   ],
   "source": [
    "my_dict = {'a': 0, 'aa': 1, 'aaah': 2, 'aahahah': 3, 'aaliyah': 4, 'aan': 5, 'aap': 6, 'aaron': 7, 'aaronmacgruder': 8, 'aaryn': 9, 'ab': 10, 'abandonado': 11, 'abbey': 12, 'abby': 13, 'abc': 14, 'abdelka': 15, 'abduction': 16, 'abdullah': 17, 'abdurahman': 18, 'abed': 19, 'abel': 20, 'aberdeen': 21, 'ability': 22, 'able': 23, 'abo': 24, 'aborted': 25, 'abortion': 26, 'abou': 27, 'abound': 28, 'about': 29, 'abouta': 30, 'above': 31, 'abraham': 32, 'abs': 33, 'absent': 34, 'absolute': 35, 'absolutely': 36}\n",
    "\n",
    "my_dict\n",
    "\n",
    "value = {i for i in my_dict if my_dict[i]==1}\n",
    "\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
