# Based on code and datasets provided by t-davidson https://github.com/t-davidson/hate-speech-and-offensive-language

# This program will extract features from the davidson dataset and use them to train a LinearSVC model. It will then test
# the data using a test dataset which is created prior.
# After this the program will take a CSV file with one column (Tweets) and will make predictions based on the text included.
# Still needs some improvements

#exec(open("./chase").read())
import time
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)
import numpy as np
from sklearn.model_selection import GridSearchCV

from sklearn.metrics import classification_report
from sklearn.linear_model import LogisticRegression
import pandas as pd
from sklearn.svm import LinearSVC
from sklearn.feature_selection import SelectFromModel
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
import nltk
from nltk.stem.porter import *
from sklearn.pipeline import Pipeline
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as VS
from textstat.textstat import *
start_time = time.time()


other_features_names = ["FKRA", "FRE","num_syllables", "avg_syl_per_word", "num_chars", "num_chars_total", \
                        "num_terms", "num_words", "num_unique_words", "vader neg","vader pos","vader neu", \
                        "vader compound", "num_hashtags", "num_mentions", "num_urls", "is_retweet"]

stopwords = nltk.corpus.stopwords.words("english")

other_exclusions = ["#ff", "ff", "rt"]
stopwords.extend(other_exclusions)

sentiment_analyzer = VS()

stemmer = PorterStemmer()


def preprocess(text_string):
    """
    Accepts a text string and replaces:
    1) urls with URLHERE
    2) lots of whitespace with one instance
    3) mentions with MENTIONHERE

    This allows us to get standardized counts of urls and mentions
    Without caring about specific people mentioned
    """
    space_pattern = '\s+'
    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'
        '[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')
    mention_regex = '@[\w\-]+'
    parsed_text = re.sub(space_pattern, ' ', text_string)
    parsed_text = re.sub(giant_url_regex, '', parsed_text)
    parsed_text = re.sub(mention_regex, '', parsed_text)
    #parsed_text = re.sub('#[\w\-]+', '',parsed_text)
    #parsed_text = parsed_text.code("utf-8", errors='ignore')
    return parsed_text

def tokenize(tweet):
    """Removes punctuation & excess whitespace, sets to lowercase,
    and stems tweets. Returns a list of stemmed tokens."""
    tweet = " ".join(re.split("[^a-zA-Z]*", tweet.lower())).strip()
    tokens = [stemmer.stem(t) for t in tweet.split()]
    return tokens

def basic_tokenize(tweet):
    """Same as tokenize but without the stemming"""
    tweet = " ".join(re.split("[^a-zA-Z.,!?]*", tweet.lower())).strip()
    return tweet.split()

def get_pos_tags(tweets):
    """Takes a list of strings (tweets) and
    returns a list of strings of (POS tags).
    """
    tweet_tags = []
    for t in tweets:
        tokens = basic_tokenize(preprocess(t))
        tags = nltk.pos_tag(tokens)
        tag_list = [x[1] for x in tags]
        #for i in range(0, len(tokens)):
        tag_str = " ".join(tag_list)
        tweet_tags.append(tag_str)
    return tweet_tags


def count_twitter_objs(text_string):
    """
    Accepts a text string and replaces:
    1) urls with URLHERE
    2) lots of whitespace with one instance
    3) mentions with MENTIONHERE
    4) hashtags with HASHTAGHERE

    This allows us to get standardized counts of urls and mentions
    Without caring about specific people mentioned.

    Returns counts of urls, mentions, and hashtags.
    """
    space_pattern = '\s+'
    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'
                       '[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')
    mention_regex = '@[\w\-]+'
    hashtag_regex = '#[\w\-]+'
    parsed_text = re.sub(space_pattern, ' ', text_string)
    parsed_text = re.sub(giant_url_regex, 'URLHERE', parsed_text)
    parsed_text = re.sub(mention_regex, 'MENTIONHERE', parsed_text)
    #parsed_text = re.sub('#', '', parsed_text) #replace the tag leave the word
    parsed_text = re.sub(hashtag_regex, 'HASHTAGHERE', parsed_text)
    return (parsed_text.count('URLHERE'), parsed_text.count('MENTIONHERE'), parsed_text.count('HASHTAGHERE'))

def class_to_name(class_label):
    """
    This function can be used to map a numeric
    feature name to a particular class.
    """
    if class_label == 0:
        return "Hate speech"
    elif class_label == 1:
        return "Offensive language"
    elif class_label == 2:
        return "Neither"
    else:
        return "No label"
def name_to_class(class_label):
    #U= unknown, R = Religion, E = Ethnicity, S = Sexuality, Y = yes blank = no, x = don't use
    if (class_label == "r") or (class_label == "e") or (class_label == "s") or (class_label == "y"):
        return "0" #Hate speech
    elif class_label == "":
        return "2"#neither
    else:
        return "x"#dont use

def other_features(tweet):
    """This function takes a string and returns a list of features.
    These include Sentiment scores, Text and Readability scores,
    as well as Twitter specific features"""
    sentiment = sentiment_analyzer.polarity_scores(tweet)

    words = preprocess(tweet)  # Get text only

    syllables = textstat.syllable_count(words)
    num_chars = sum(len(w) for w in words)
    num_chars_total = len(tweet)
    num_terms = len(tweet.split())
    num_words = len(words.split())
    avg_syl = round(float((syllables + 0.001)) / float(num_words + 0.001), 4)
    num_unique_terms = len(set(words.split()))

    ###Modified FK grade, where avg words per sentence is just num words/1
    FKRA = round(float(0.39 * float(num_words) / 1.0) + float(11.8 * avg_syl) - 15.59, 1)
    ##Modified FRE score, where sentence fixed to 1
    FRE = round(206.835 - 1.015 * (float(num_words) / 1.0) - (84.6 * float(avg_syl)), 2)

    twitter_objs = count_twitter_objs(tweet)
    retweet = 0
    if "rt" in words:
        retweet = 1
    features = [FKRA, FRE, syllables, avg_syl, num_chars, num_chars_total, num_terms, num_words,
                num_unique_terms, sentiment['neg'], sentiment['pos'], sentiment['neu'], sentiment['compound'],
                twitter_objs[2], twitter_objs[1],
                twitter_objs[0], retweet]
    # features = pandas.DataFrame(features)
    return features


def get_feature_array(tweets):
    feats = []
    for t in tweets:
        feats.append(other_features(t))
    return np.array(feats)


if __name__ == '__main__':
    print("hello world")
    dataset = pd.read_csv("../../../data/chaseanddavidsons.csv") #currently only a 500 subset
    #dataset = pd.read_csv("davidsonxchaseHate.csv", encoding="latin-1")
    #dataset = pd.read_csv("chaseTrain.csv", encoding="latin-1")
    chaseDataset = pd.read_csv("../../../data/train1.csv", encoding="latin-1") #This reads chase tweets from a CSV file that are used later


    #print(dataset.describe())
    print(dataset.columns)
    tweets = dataset.tweet #store the tweets in an easier way
    chaseTweets = chaseDataset.tweet
    print(len(tweets), " tweets to classify")
    train, test, y_train, y_test = train_test_split(dataset.tweet, dataset.hate_speech, test_size=0.25, random_state=4)
    #train, test, y_train, y_test = train_test_split(dataset.tweet, dataset.score, test_size=0.25)
    print(len(train), "length of train")
    print(len(test), "length of test")

    #Vectoriser
    vectorizer = TfidfVectorizer(
        tokenizer=tokenize,
        preprocessor=preprocess,
        ngram_range=(1, 3),
        stop_words=stopwords,
        use_idf=True,
        smooth_idf=False,
        norm=None,
        decode_error='replace',
        max_features=10000,
        min_df=5,
        max_df=0.75
    )

    #text_clf = text_clf.fit(train,y_train)
    #print(test)
    tfidf = vectorizer.fit_transform(tweets).toarray()
    vocab = {v:i for i, v in enumerate(vectorizer.get_feature_names())}
    idf_vals = vectorizer.idf_
    idf_dict = {i: idf_vals[i] for i in vocab.values()}

    tweet_tags = []
    for t in tweets:
        tokens = basic_tokenize(preprocess(t))
        tags = nltk.pos_tag(tokens)
        tag_list = [x[1] for x in tags]
        tag_str = " ".join(tag_list)
        tweet_tags.append(tag_str)

    pos_vectorizer = TfidfVectorizer(
        tokenizer=None,
        lowercase=False,
        preprocessor=None,
        ngram_range=(1, 3),
        stop_words=None,
        use_idf=False,
        smooth_idf=False,
        norm=None,
        decode_error='replace',
        max_features=5000,
        min_df=5,
        max_df=0.75,
    )

    #text_clf = Pipeline([#('vect', vectorizer),
     #                    ('tfidf', vectorizer),
      #                   ('clf', LinearSVC(class_weight='balanced', C=0.01, penalty='l2', loss='squared_hinge', multi_class='ovr'))
       #                  ])
    pos = pos_vectorizer.fit_transform(pd.Series(tweet_tags)).toarray()
    pos_vocab = {v: i for i, v in enumerate(pos_vectorizer.get_feature_names())}

    feats = get_feature_array(tweets)

    M = np.concatenate([tfidf, pos, feats], axis=1)
    print(M.shape)

    variables = [''] * len(vocab)
    for k, v in vocab.items():
        variables[v] = k

    pos_variables = [''] * len(pos_vocab)
    for k, v in pos_vocab.items():
        pos_variables[v] = k

    feature_names = variables + pos_variables + other_features_names

    X = pd.DataFrame(M)
    y = dataset['class'].astype(int)
    select = SelectFromModel(LogisticRegression(class_weight='balanced',penalty="l1",C=0.01))
    X1 = select.fit_transform(X,y)
    model = LinearSVC(class_weight='balanced', C=0.01, penalty='l2', loss='squared_hinge', multi_class='ovr').fit(X1, y)
    model = LogisticRegression(class_weight='balanced', penalty='l2', C=0.01).fit(X1, y)


    y_preds = model.predict(X1)

    print(classification_report( y, y_preds ))

    print("--- %s seconds ---" % (time.time() - start_time))
    parameters = {'vect__ngram_range': [(1, 1), (1, 2)],
                  'tfidf__use_idf': (True, False),
                  'clf__alpha': (1e-2, 1e-3),
                  }

    #print(gs_clf.predict(['God is love']))
    #print("Running classification model...")
    print("""\t----------------------
    Predicting tweets
    ----------------------""")
    with open("/Users/David/spur/chase/output/predictions.csv", 'w') as output:
        output.write("tweet,truth,prediction\n")
        for i,t in enumerate(tweets):
            #print(t)
            t = re.sub('\n|,', '', t)
            output.write(t + ',' + class_to_name(y[i])+','+class_to_name(y_preds[i]) + '\n')
            print(t + " :::::::::: " + class_to_name(y[i]))


    #inDataset = pd.read_csv("../../../data/labeled_data.csv")
    # intweets = inDataset.tweet

'''
    tfidf = vectorizer.fit_transform(chaseTweets).toarray()
    vocab = {v:i for i, v in enumerate(vectorizer.get_feature_names())}
    idf_vals = vectorizer.idf_
    idf_dict = {i: idf_vals[i] for i in vocab.values()}

    tweet_tags = []
    for t in chaseTweets:
        tokens = basic_tokenize(preprocess(t))
        tags = nltk.pos_tag(tokens)
        tag_list = [x[1] for x in tags]
        tag_str = " ".join(tag_list)
        tweet_tags.append(tag_str)
    pos = pos_vectorizer.fit_transform(pd.Series(tweet_tags)).toarray()
    pos_vocab = {v: i for i, v in enumerate(pos_vectorizer.get_feature_names())}

    feats = get_feature_array(chaseTweets)

    M = np.concatenate([tfidf, pos, feats], axis=1)
    print(M.shape)

    variables = [''] * len(vocab)
    for k, v in vocab.items():
        variables[v] = k

    pos_variables = [''] * len(pos_vocab)
    for k, v in pos_vocab.items():
        pos_variables[v] = k

    feature_names = variables + pos_variables + other_features_names
    X = pd.DataFrame(M)
    y = chaseDataset['score'].astype(int)
    X1 = select.fit_transform(X,y)
    model = LinearSVC(class_weight='balanced', C=0.01, penalty='l2', loss='squared_hinge', multi_class='ovr').fit(X1, y)
    model = LogisticRegression(class_weight='balanced', penalty='l2', C=0.01).fit(X1, y)

    y_preds = model.predict(X1)

    print("""\t----------------------
    Predicting chase tweets
    ----------------------""")
    with open("/Users/David/spur/chase/output/predictions.csv", 'w') as output:
        output.write("tweet,classification\n")
        for i, t in enumerate(chaseTweets):
            # print(t)
            t = re.sub('\n|,','',t)
            output.write(t + ',' + class_to_name(y_preds[i]) + '\n')
            print(t + " :::::::::: " + class_to_name(y_preds[i]))
'''