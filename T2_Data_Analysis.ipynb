{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pdb\n",
    "import codecs\n",
    "import pdb\n",
    "import pandas as pd\n",
    "\n",
    "def get_t2_data():\n",
    "    tweets = []\n",
    "    files = ['racism.json', 'neither.json', 'sexism.json']\n",
    "    for file in files:\n",
    "        with codecs.open('/home/sjonnal3/Hate_Speech_Detection/Applied_Machine_Learning/tweet_data/' + file, 'r', encoding='utf-8') as f:\n",
    "            data = f.readlines()\n",
    "        for line in data:\n",
    "            tweet_full = json.loads(line)\n",
    "            tweets.append({\n",
    "                'id': tweet_full['id'],\n",
    "                'text': tweet_full['text'].lower(),\n",
    "                'label': tweet_full['Annotation'],\n",
    "                'name': tweet_full['user']['name'].split()[0]\n",
    "                })\n",
    "\n",
    "    #pdb.set_trace()\n",
    "    return tweets\n",
    "\n",
    "\n",
    "\n",
    "tweets = get_t2_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data = pd.DataFrame(tweets)\n",
    "\n",
    "temp_data['label'] = temp_data['label'].replace([\"sexism\",\"racism\"],0)\n",
    "temp_data['label'] = temp_data['label'].replace([\"none\"],1)\n",
    "\n",
    "temp_data['label'].value_counts()\n",
    "\n",
    "temp_data.columns = ['id','tweet','class','name']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data.to_csv('/home/sjonnal3/Hate_Speech_Detection/Applied_Machine_Learning/chase-master/data/ml/public/dt/racism_sexism_2classes_only.csv', sep=',', encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('/home/sjonnal3/Hate_Speech_Detection/Applied_Machine_Learning/chase-master/data/ml/public/dt/racism_sexism_2classes_only.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14992</th>\n",
       "      <td>#yesallmen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0  id tweet  class name\n",
       "14992  #yesallmen  NaN   NaN    NaN  NaN"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data[raw_data['tweet'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_data['tweet'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method to_csv in module pandas.core.generic:\n",
      "\n",
      "to_csv(path_or_buf: 'FilePath | WriteBuffer[bytes] | WriteBuffer[str] | None' = None, sep: 'str' = ',', na_rep: 'str' = '', float_format: 'str | Callable | None' = None, columns: 'Sequence[Hashable] | None' = None, header: 'bool_t | list[str]' = True, index: 'bool_t' = True, index_label: 'IndexLabel | None' = None, mode: 'str' = 'w', encoding: 'str | None' = None, compression: 'CompressionOptions' = 'infer', quoting: 'int | None' = None, quotechar: 'str' = '\"', lineterminator: 'str | None' = None, chunksize: 'int | None' = None, date_format: 'str | None' = None, doublequote: 'bool_t' = True, escapechar: 'str | None' = None, decimal: 'str' = '.', errors: 'str' = 'strict', storage_options: 'StorageOptions' = None) -> 'str | None' method of pandas.core.frame.DataFrame instance\n",
      "    Write object to a comma-separated values (csv) file.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    path_or_buf : str, path object, file-like object, or None, default None\n",
      "        String, path object (implementing os.PathLike[str]), or file-like\n",
      "        object implementing a write() function. If None, the result is\n",
      "        returned as a string. If a non-binary file object is passed, it should\n",
      "        be opened with `newline=''`, disabling universal newlines. If a binary\n",
      "        file object is passed, `mode` might need to contain a `'b'`.\n",
      "    \n",
      "        .. versionchanged:: 1.2.0\n",
      "    \n",
      "           Support for binary file objects was introduced.\n",
      "    \n",
      "    sep : str, default ','\n",
      "        String of length 1. Field delimiter for the output file.\n",
      "    na_rep : str, default ''\n",
      "        Missing data representation.\n",
      "    float_format : str, Callable, default None\n",
      "        Format string for floating point numbers. If a Callable is given, it takes\n",
      "        precedence over other numeric formatting parameters, like decimal.\n",
      "    columns : sequence, optional\n",
      "        Columns to write.\n",
      "    header : bool or list of str, default True\n",
      "        Write out the column names. If a list of strings is given it is\n",
      "        assumed to be aliases for the column names.\n",
      "    index : bool, default True\n",
      "        Write row names (index).\n",
      "    index_label : str or sequence, or False, default None\n",
      "        Column label for index column(s) if desired. If None is given, and\n",
      "        `header` and `index` are True, then the index names are used. A\n",
      "        sequence should be given if the object uses MultiIndex. If\n",
      "        False do not print fields for index names. Use index_label=False\n",
      "        for easier importing in R.\n",
      "    mode : str, default 'w'\n",
      "        Python write mode. The available write modes are the same as\n",
      "        :py:func:`open`.\n",
      "    encoding : str, optional\n",
      "        A string representing the encoding to use in the output file,\n",
      "        defaults to 'utf-8'. `encoding` is not supported if `path_or_buf`\n",
      "        is a non-binary file object.\n",
      "    compression : str or dict, default 'infer'\n",
      "        For on-the-fly compression of the output data. If 'infer' and 'path_or_buf' is\n",
      "        path-like, then detect compression from the following extensions: '.gz',\n",
      "        '.bz2', '.zip', '.xz', '.zst', '.tar', '.tar.gz', '.tar.xz' or '.tar.bz2'\n",
      "        (otherwise no compression).\n",
      "        Set to ``None`` for no compression.\n",
      "        Can also be a dict with key ``'method'`` set\n",
      "        to one of {``'zip'``, ``'gzip'``, ``'bz2'``, ``'zstd'``, ``'tar'``} and other\n",
      "        key-value pairs are forwarded to\n",
      "        ``zipfile.ZipFile``, ``gzip.GzipFile``,\n",
      "        ``bz2.BZ2File``, ``zstandard.ZstdCompressor`` or\n",
      "        ``tarfile.TarFile``, respectively.\n",
      "        As an example, the following could be passed for faster compression and to create\n",
      "        a reproducible gzip archive:\n",
      "        ``compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}``.\n",
      "    \n",
      "            .. versionadded:: 1.5.0\n",
      "                Added support for `.tar` files.\n",
      "    \n",
      "        .. versionchanged:: 1.0.0\n",
      "    \n",
      "           May now be a dict with key 'method' as compression mode\n",
      "           and other entries as additional compression options if\n",
      "           compression mode is 'zip'.\n",
      "    \n",
      "        .. versionchanged:: 1.1.0\n",
      "    \n",
      "           Passing compression options as keys in dict is\n",
      "           supported for compression modes 'gzip', 'bz2', 'zstd', and 'zip'.\n",
      "    \n",
      "        .. versionchanged:: 1.2.0\n",
      "    \n",
      "            Compression is supported for binary file objects.\n",
      "    \n",
      "        .. versionchanged:: 1.2.0\n",
      "    \n",
      "            Previous versions forwarded dict entries for 'gzip' to\n",
      "            `gzip.open` instead of `gzip.GzipFile` which prevented\n",
      "            setting `mtime`.\n",
      "    \n",
      "    quoting : optional constant from csv module\n",
      "        Defaults to csv.QUOTE_MINIMAL. If you have set a `float_format`\n",
      "        then floats are converted to strings and thus csv.QUOTE_NONNUMERIC\n",
      "        will treat them as non-numeric.\n",
      "    quotechar : str, default '\\\"'\n",
      "        String of length 1. Character used to quote fields.\n",
      "    lineterminator : str, optional\n",
      "        The newline character or character sequence to use in the output\n",
      "        file. Defaults to `os.linesep`, which depends on the OS in which\n",
      "        this method is called ('\\\\n' for linux, '\\\\r\\\\n' for Windows, i.e.).\n",
      "    \n",
      "        .. versionchanged:: 1.5.0\n",
      "    \n",
      "            Previously was line_terminator, changed for consistency with\n",
      "            read_csv and the standard library 'csv' module.\n",
      "    \n",
      "    chunksize : int or None\n",
      "        Rows to write at a time.\n",
      "    date_format : str, default None\n",
      "        Format string for datetime objects.\n",
      "    doublequote : bool, default True\n",
      "        Control quoting of `quotechar` inside a field.\n",
      "    escapechar : str, default None\n",
      "        String of length 1. Character used to escape `sep` and `quotechar`\n",
      "        when appropriate.\n",
      "    decimal : str, default '.'\n",
      "        Character recognized as decimal separator. E.g. use ',' for\n",
      "        European data.\n",
      "    errors : str, default 'strict'\n",
      "        Specifies how encoding and decoding errors are to be handled.\n",
      "        See the errors argument for :func:`open` for a full list\n",
      "        of options.\n",
      "    \n",
      "        .. versionadded:: 1.1.0\n",
      "    \n",
      "    storage_options : dict, optional\n",
      "        Extra options that make sense for a particular storage connection, e.g.\n",
      "        host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n",
      "        are forwarded to ``urllib.request.Request`` as header options. For other\n",
      "        URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are\n",
      "        forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n",
      "        details, and for more examples on storage options refer `here\n",
      "        <https://pandas.pydata.org/docs/user_guide/io.html?\n",
      "        highlight=storage_options#reading-writing-remote-files>`_.\n",
      "    \n",
      "        .. versionadded:: 1.2.0\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    None or str\n",
      "        If path_or_buf is None, returns the resulting csv format as a\n",
      "        string. Otherwise returns None.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    read_csv : Load a CSV file into a DataFrame.\n",
      "    to_excel : Write DataFrame to an Excel file.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = pd.DataFrame({'name': ['Raphael', 'Donatello'],\n",
      "    ...                    'mask': ['red', 'purple'],\n",
      "    ...                    'weapon': ['sai', 'bo staff']})\n",
      "    >>> df.to_csv(index=False)\n",
      "    'name,mask,weapon\\nRaphael,red,sai\\nDonatello,purple,bo staff\\n'\n",
      "    \n",
      "    Create 'out.zip' containing 'out.csv'\n",
      "    \n",
      "    >>> compression_opts = dict(method='zip',\n",
      "    ...                         archive_name='out.csv')  # doctest: +SKIP\n",
      "    >>> df.to_csv('out.zip', index=False,\n",
      "    ...           compression=compression_opts)  # doctest: +SKIP\n",
      "    \n",
      "    To write a csv file to a new folder or nested folder you will first\n",
      "    need to create it using either Pathlib or os:\n",
      "    \n",
      "    >>> from pathlib import Path  # doctest: +SKIP\n",
      "    >>> filepath = Path('folder/subfolder/out.csv')  # doctest: +SKIP\n",
      "    >>> filepath.parent.mkdir(parents=True, exist_ok=True)  # doctest: +SKIP\n",
      "    >>> df.to_csv(filepath)  # doctest: +SKIP\n",
      "    \n",
      "    >>> import os  # doctest: +SKIP\n",
      "    >>> os.makedirs('folder/subfolder', exist_ok=True)  # doctest: +SKIP\n",
      "    >>> df.to_csv('folder/subfolder/out.csv')  # doctest: +SKIP\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(temp_data.to_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    11501\n",
      "0     5406\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "raw_data = pd.read_csv('/home/sjonnal3/Hate_Speech_Detection/Applied_Machine_Learning/chase-master/data/ml/public/dt/racism_sexism_2classes_only.csv', sep=',', encoding=\"utf-8\")\n",
    "\n",
    "print(raw_data['class'].value_counts())\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "tweets = raw_data['tweet']\n",
    "eng_stop_words = stopwords.words(\"english\")\n",
    "\n",
    "tweet_without_stopwords = tweets.apply(lambda x: ' '.join([word for word in x.split() if word not in (eng_stop_words)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk import PorterStemmer, WordNetLemmatizer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as VS\n",
    "\n",
    "sentiment_analyzer = VS()\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "other_exclusions = [\"#ff\", \"ff\", \"rt\"]\n",
    "stopwords.extend(other_exclusions)\n",
    "\n",
    "\n",
    "\n",
    "################################# Modifed the code from Char to Word level from Pinkesh paper ################################################\n",
    "\n",
    "import sys\n",
    "import re\n",
    "from string import punctuation\n",
    "# from preprocess_twitter import tokenize as tokenizer_g\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "FLAGS = re.MULTILINE | re.DOTALL\n",
    "\n",
    "def hashtag(text):\n",
    "    text = text.group()\n",
    "    hashtag_body = text[1:]\n",
    "    if hashtag_body.isupper():\n",
    "        result = u\"<hashtag> {} <allcaps>\".format(hashtag_body)\n",
    "    else:\n",
    "        result = \" \".join([\"<hashtag>\"] + re.split(\"(?=[A-Z])\", hashtag_body, flags=FLAGS))\n",
    "    return result\n",
    "\n",
    "def allcaps(text):\n",
    "    text = text.group()\n",
    "    return text.lower() + \" <allcaps>\"\n",
    "\n",
    "\n",
    "def tokenizer_g(text):\n",
    "    # Different regex parts for smiley faces\n",
    "    eyes = r\"[8:=;]\"\n",
    "    nose = r\"['`\\-]?\"\n",
    "\n",
    "    # function so code less repetitive\n",
    "    def re_sub(pattern, repl):\n",
    "        return re.sub(pattern, repl, text, flags=FLAGS)\n",
    "\n",
    "    text = re_sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \"<url>\")\n",
    "    text = re_sub(r\"/\",\" / \")\n",
    "    text = re_sub(r\"@\\w+\", \"<user>\")\n",
    "    text = re_sub(r\"{}{}[)dD]+|[)dD]+{}{}\".format(eyes, nose, nose, eyes), \"<smile>\")\n",
    "    text = re_sub(r\"{}{}p+\".format(eyes, nose), \"<lolface>\")\n",
    "    text = re_sub(r\"{}{}\\(+|\\)+{}{}\".format(eyes, nose, nose, eyes), \"<sadface>\")\n",
    "    text = re_sub(r\"{}{}[\\/|l*]\".format(eyes, nose), \"<neutralface>\")\n",
    "    text = re_sub(r\"<3\",\"<heart>\")\n",
    "    text = re_sub(r\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", \"<number>\")\n",
    "    text = re_sub(r\"#\\S+\", hashtag)\n",
    "    text = re_sub(r\"([!?.]){2,}\", r\"\\1 <repeat>\")\n",
    "    text = re_sub(r\"\\b(\\S*?)(.)\\2{2,}\\b\", r\"\\1\\2 <elong>\")\n",
    "\n",
    "    ## -- I just don't understand why the Ruby script adds <allcaps> to everything so I limited the selection.\n",
    "    # text = re_sub(r\"([^a-z0-9()<>'`\\-]){2,}\", allcaps)\n",
    "    text = re_sub(r\"([A-Z]){2,}\", allcaps)\n",
    "\n",
    "    return text.lower()\n",
    "\n",
    "def glove_tokenize(text):\n",
    "    text = tokenizer_g(text)\n",
    "    text = ''.join([c for c in text if c not in punctuation])\n",
    "    words = text.split()\n",
    "    # words = [word for word in words if word not in STOPWORDS]\n",
    "    return words\n",
    "\n",
    "################################################################################################################\n",
    "\n",
    "\n",
    "# stem_or_lemma: 0 - apply porter's stemming; 1: apply lemmatization; 2: neither\n",
    "# -set to 0 to reproduce Davidson. However, note that because a different stemmer is used, results could be\n",
    "# sightly different\n",
    "# -set to 2 will do 'basic_tokenize' as in Davidson\n",
    "def tokenize(tweet, stem_or_lemma=0):\n",
    "    \"\"\"Removes punctuation & excess whitespace, sets to lowercase,\n",
    "    and normalizes tweets. Returns a list of stemmed tokens.\"\"\"\n",
    "    tweet = \" \".join(re.split(\"[^a-zA-Z]*\", tweet.lower())).strip()\n",
    "    if stem_or_lemma==0:\n",
    "        tokens = [stemmer.stem(t) for t in tweet.split()]\n",
    "    elif stem_or_lemma==1:\n",
    "        tokens=[lemmatizer.lemmatize(t) for t in tweet.split()]\n",
    "    else:\n",
    "        tokens = [t for t in tweet.split()] #this is basic_tokenize in TD's original code\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# tweets should have been preprocessed to the clean/right format before passing to this method\n",
    "def get_pos_tags(tweets):\n",
    "    \"\"\"Takes a list of strings (tweets) and\n",
    "    returns a list of strings of (POS tags).\n",
    "    \"\"\"\n",
    "    tweet_tags = []\n",
    "    for t in tweets:\n",
    "        tokens = tokenize(t, 2)\n",
    "        tags = nltk.pos_tag(tokens)\n",
    "        tag_list = [x[1] for x in tags]\n",
    "        #for i in range(0, len(tokens)):\n",
    "        tag_str = \" \".join(tag_list)\n",
    "        tweet_tags.append(tag_str)\n",
    "    return tweet_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import enchant\n",
    "import splitter\n",
    "\n",
    "d = enchant.Dict('en_UK')\n",
    "dus = enchant.Dict('en_US')\n",
    "space_pattern = '\\s+'\n",
    "giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "mention_regex = '@[\\w\\-]+'\n",
    "emoji_regex = '&#[0-9]{4,6};'\n",
    "\n",
    "#This is the original preprocess method from Davidson\n",
    "def preprocess(text_string):\n",
    "    \"\"\"\n",
    "    Accepts a text string and replaces:\n",
    "    1) urls with URLHERE\n",
    "    2) lots of whitespace with one instance\n",
    "    3) mentions with MENTIONHERE\n",
    "\n",
    "    This allows us to get standardized counts of urls and mentions\n",
    "    Without caring about specific people mentioned\n",
    "    \"\"\"\n",
    "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "    parsed_text = re.sub(giant_url_regex, '', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, '', parsed_text)\n",
    "    parsed_text = re.sub('RT','', parsed_text) #Some RTs have !!!!! in front of them\n",
    "    parsed_text = re.sub(emoji_regex,'',parsed_text) #remove emojis from the text\n",
    "    parsed_text = re.sub('…','',parsed_text) #Remove the special ending character is truncated\n",
    "    #parsed_text = re.sub('#[\\w\\-]+', '',parsed_text)\n",
    "    #parsed_text = parsed_text.code(\"utf-8\", errors='ignore')\n",
    "    return parsed_text\n",
    "\n",
    "\n",
    "def preprocess_clean(text_string, remove_hashtags=True, remove_special_chars=True):\n",
    "    # Clean a string down to just text\n",
    "    text_string=preprocess(text_string)\n",
    "\n",
    "    parsed_text = preprocess(text_string)\n",
    "    parsed_text = parsed_text.lower()\n",
    "    parsed_text = re.sub('\\'', '', parsed_text)\n",
    "    parsed_text = re.sub('|', '', parsed_text)\n",
    "    parsed_text = re.sub(':', '', parsed_text)\n",
    "    parsed_text = re.sub(',', '', parsed_text)\n",
    "    parsed_text = re.sub(';', '.', parsed_text)\n",
    "    parsed_text = re.sub('&amp', '', parsed_text)\n",
    "\n",
    "    if remove_hashtags:\n",
    "        parsed_text = re.sub('#[\\w\\-]+', '',parsed_text)\n",
    "    if remove_special_chars:\n",
    "        #parsed_text = re.sub('(\\!|\\?)+','.',parsed_text) #find one or more of special char in a row, replace with one '.'\n",
    "        parsed_text = re.sub('(\\!|\\?)+','',parsed_text)\n",
    "    return parsed_text\n",
    "\n",
    "def strip_hashtags(text):\n",
    "    text = preprocess_clean(text,False,True)\n",
    "    hashtags = re.findall('#[\\w\\-]+', text)\n",
    "    for tag in hashtags:\n",
    "        cleantag = tag[1:]\n",
    "        if d.check(cleantag) or dus.check(cleantag):\n",
    "            text = re.sub(tag,cleantag,text)\n",
    "            pass\n",
    "        else:\n",
    "            hashtagSplit = \"\"\n",
    "            for word in splitter.split(cleantag.lower(),'en_US'):\n",
    "                hashtagSplit = hashtagSplit + word + \" \"\n",
    "            text = re.sub(tag,hashtagSplit,text)\n",
    "    #print(text)\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sjonnal3/.local/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import functools\n",
    "\n",
    "\n",
    "word_vectorizer = CountVectorizer(\n",
    "        # vectorizer = sklearn.feature_extraction.text.CountVectorizer(\n",
    "        # tokenizer=functools.partial(nlp.tokenize, stem_or_lemma=normalize_option),\n",
    "        tokenizer = functools.partial(glove_tokenize),\n",
    "        preprocessor=strip_hashtags,\n",
    "        ngram_range=(1, 1),\n",
    "        # stop_words=nlp.stopwords,  # We do better when we keep stopwords\n",
    "        # stop_words = None,\n",
    "        decode_error='replace',\n",
    "        max_features=50000\n",
    "        # min_df=2,\n",
    "        # max_df=0.5\n",
    "    )\n",
    "\n",
    "counts = word_vectorizer.fit_transform(tweet_without_stopwords).toarray()\n",
    "\n",
    "vocab = {v: i for i, v in enumerate(word_vectorizer.get_feature_names())}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0 drasko said impressed girls... 1 drasko cook...\n",
       "1        0 drasko said impressed girls... 1 drasko cook...\n",
       "2        0 drasko said impressed girls... 1 drasko cook...\n",
       "3        0 drasko said impressed girls... 1 drasko cook...\n",
       "4        0 drasko said impressed girls... 1 drasko cook...\n",
       "                               ...                        \n",
       "16902    0 drasko said impressed girls... 1 drasko cook...\n",
       "16903    0 drasko said impressed girls... 1 drasko cook...\n",
       "16904    0 drasko said impressed girls... 1 drasko cook...\n",
       "16905    0 drasko said impressed girls... 1 drasko cook...\n",
       "16906    0 drasko said impressed girls... 1 drasko cook...\n",
       "Name: tweet, Length: 16907, dtype: object"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = raw_data['tweet']\n",
    "\n",
    "eng_stop_words = stopwords.words(\"english\")\n",
    "\n",
    "tweet_without_stopwords = tweets.apply(lambda x: ' '.join([word for word in x.split() if word not in (eng_stop_words)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0        572342978255048705\\n1        57234149...</td>\n",
       "      <td>0        so drasko just said he was impressed ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0        thefoxbandit\\n1            patricia\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0        572342978255048705\\n1        57234149...</td>\n",
       "      <td>0        so drasko just said he was impressed ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0        thefoxbandit\\n1            patricia\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0        572342978255048705\\n1        57234149...</td>\n",
       "      <td>0        so drasko just said he was impressed ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0        thefoxbandit\\n1            patricia\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0        572342978255048705\\n1        57234149...</td>\n",
       "      <td>0        so drasko just said he was impressed ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0        thefoxbandit\\n1            patricia\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0        572342978255048705\\n1        57234149...</td>\n",
       "      <td>0        so drasko just said he was impressed ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0        thefoxbandit\\n1            patricia\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16902</th>\n",
       "      <td>16902</td>\n",
       "      <td>0        572342978255048705\\n1        57234149...</td>\n",
       "      <td>0        so drasko just said he was impressed ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0        thefoxbandit\\n1            patricia\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16903</th>\n",
       "      <td>16903</td>\n",
       "      <td>0        572342978255048705\\n1        57234149...</td>\n",
       "      <td>0        so drasko just said he was impressed ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0        thefoxbandit\\n1            patricia\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16904</th>\n",
       "      <td>16904</td>\n",
       "      <td>0        572342978255048705\\n1        57234149...</td>\n",
       "      <td>0        so drasko just said he was impressed ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0        thefoxbandit\\n1            patricia\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16905</th>\n",
       "      <td>16905</td>\n",
       "      <td>0        572342978255048705\\n1        57234149...</td>\n",
       "      <td>0        so drasko just said he was impressed ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0        thefoxbandit\\n1            patricia\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16906</th>\n",
       "      <td>16906</td>\n",
       "      <td>0        572342978255048705\\n1        57234149...</td>\n",
       "      <td>0        so drasko just said he was impressed ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0        thefoxbandit\\n1            patricia\\n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16907 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                                 id  \\\n",
       "0               0  0        572342978255048705\\n1        57234149...   \n",
       "1               1  0        572342978255048705\\n1        57234149...   \n",
       "2               2  0        572342978255048705\\n1        57234149...   \n",
       "3               3  0        572342978255048705\\n1        57234149...   \n",
       "4               4  0        572342978255048705\\n1        57234149...   \n",
       "...           ...                                                ...   \n",
       "16902       16902  0        572342978255048705\\n1        57234149...   \n",
       "16903       16903  0        572342978255048705\\n1        57234149...   \n",
       "16904       16904  0        572342978255048705\\n1        57234149...   \n",
       "16905       16905  0        572342978255048705\\n1        57234149...   \n",
       "16906       16906  0        572342978255048705\\n1        57234149...   \n",
       "\n",
       "                                                   tweet  class  \\\n",
       "0      0        so drasko just said he was impressed ...      0   \n",
       "1      0        so drasko just said he was impressed ...      0   \n",
       "2      0        so drasko just said he was impressed ...      0   \n",
       "3      0        so drasko just said he was impressed ...      0   \n",
       "4      0        so drasko just said he was impressed ...      0   \n",
       "...                                                  ...    ...   \n",
       "16902  0        so drasko just said he was impressed ...      0   \n",
       "16903  0        so drasko just said he was impressed ...      0   \n",
       "16904  0        so drasko just said he was impressed ...      0   \n",
       "16905  0        so drasko just said he was impressed ...      0   \n",
       "16906  0        so drasko just said he was impressed ...      0   \n",
       "\n",
       "                                                    name  \n",
       "0      0        thefoxbandit\\n1            patricia\\n...  \n",
       "1      0        thefoxbandit\\n1            patricia\\n...  \n",
       "2      0        thefoxbandit\\n1            patricia\\n...  \n",
       "3      0        thefoxbandit\\n1            patricia\\n...  \n",
       "4      0        thefoxbandit\\n1            patricia\\n...  \n",
       "...                                                  ...  \n",
       "16902  0        thefoxbandit\\n1            patricia\\n...  \n",
       "16903  0        thefoxbandit\\n1            patricia\\n...  \n",
       "16904  0        thefoxbandit\\n1            patricia\\n...  \n",
       "16905  0        thefoxbandit\\n1            patricia\\n...  \n",
       "16906  0        thefoxbandit\\n1            patricia\\n...  \n",
       "\n",
       "[16907 rows x 5 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
